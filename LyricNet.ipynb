{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def openFile(path):\n",
    "    f = open(path, \"r\")\n",
    "    return f.read() \n",
    "\n",
    "def imshow(img,text,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()\n",
    "    \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hdim, vocab_size):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.hdim = hdim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hdim, num_layers=2)#, bidirectional=True)\n",
    "        \n",
    "        self.hidden = self.initHidden(self.hdim)\n",
    "        self.output = nn.Linear(self.hdim, 1)\n",
    "\n",
    "        \n",
    "    def initHidden(self, dim):\n",
    "        return (autograd.Variable(torch.zeros(2, 1, dim)).cuda(),\n",
    "        autograd.Variable(torch.zeros(2, 1, dim)).cuda())\n",
    "        \n",
    "    def forward_once(self, lyric):\n",
    "        embeds = self.embeddings(lyric)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(lyric), 1, -1), self.hidden1)\n",
    "        scores = self.output(lstm_out.view(len(lyric), -1))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "#         print(output1, output2)\n",
    "        out1 = output1[len(output1)-1]\n",
    "        out2 = output2[len(output2)-1]\n",
    "#         return F.softmax(-torch.abs(out1-out2))\n",
    "        return torch.abs(out1 - out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricDataset(Dataset):\n",
    "    def __init__(self, pathToData, numClasses, should_invert=True):\n",
    "        self.pathToData = pathToData\n",
    "        self.data = [path for path in os.listdir(self.pathToData) if os.path.isdir(self.pathToData + \"/\" + path)]\n",
    "        self.should_invert = should_invert\n",
    "        self.numClasses = numClasses\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        labels = self.data\n",
    "        sameClass = random.randint(0,1)\n",
    "        song1 = None\n",
    "        song2 = None\n",
    "        if sameClass: \n",
    "            label = random.randint(0, self.numClasses-1)\n",
    "            classPath = self.pathToData + \"/\" + labels[label]\n",
    "            songs = [path for path in os.listdir(classPath) if not path.startswith(\".\")]\n",
    "            index1 = random.randint(0,len(songs)-1)\n",
    "            s1Path = classPath + \"/\" + songs[index1]\n",
    "            index2 = index1 \n",
    "            while index2 == index1:\n",
    "                index2 = random.randint(0,len(songs)-1)\n",
    "            s2Path = classPath + \"/\" + songs[index2]\n",
    "#             print(songs[index1], songs[index2])\n",
    "\n",
    "        else:\n",
    "            label1 = random.randint(0, self.numClasses-1)\n",
    "            label2 = label1\n",
    "            while label2 == label1:\n",
    "                label2 = random.randint(0, self.numClasses-1)\n",
    "            classPath1 = self.pathToData + \"/\" + labels[label1]\n",
    "            songs1 = [path for path in os.listdir(classPath1) if not path.startswith(\".\")]\n",
    "            index1 = random.randint(0,len(songs1)-1)\n",
    "            s1Path = classPath1 + \"/\" + songs1[index1]\n",
    "            classPath2 = self.pathToData + \"/\" + labels[label2]\n",
    "            songs2 = [path for path in os.listdir(classPath2) if not path.startswith(\".\")]\n",
    "            index2 = random.randint(0,len(songs2)-1)\n",
    "            s2Path = classPath2 + \"/\" + songs2[index2]\n",
    "#             print(songs1[index1], songs2[index2])\n",
    "        s1 = openFile(s1Path)\n",
    "        lyric1 = [convertForDict(word) for word in s1.split(\" \")]\n",
    "        s2 = openFile(s2Path)\n",
    "        lyric2 = [convertForDict(word) for word in s2.split(\" \")]\n",
    "        return lyric1, lyric2, sameClass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([len(os.listdir(self.pathToData + \"/\" + p)) for p in os.listdir(self.pathToData) if p != \".DS_Store\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        return torch.abs(torch.mean(torch.abs(output1-output2))- label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LyricDataset at 0x7f9aade6dda0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/home/ubuntu/LyricNet/train\"\n",
    "dataset = LyricDataset(PATH, 2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertForDict(word):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    word = pattern.sub('', word)\n",
    "    return word.lower()\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor).cuda()\n",
    "\n",
    "word_to_ix = {}\n",
    "labels = dataset.data\n",
    "for artist in labels:\n",
    "    for song in os.listdir(dataset.pathToData + \"/\" + artist):\n",
    "        for word in openFile(dataset.pathToData + \"/\" + artist + \"/\" + song).split(\" \"):\n",
    "            word = convertForDict(word)\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = []\n",
    "avg_loss = []\n",
    "iteration = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.9922123551368713\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0006372292991727591\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.980278491973877\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.8937227725982666\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.9560503363609314\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.8930591344833374\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.002573743462562561\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.9627166986465454\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0001058576672221534\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0074831657111644745\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2d0a09c04833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EDIM = 512\n",
    "HDIM = 512\n",
    "\n",
    "model = SiameseLSTM(EDIM, HDIM, len(word_to_ix)).cuda()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for epoch in range(5):\n",
    "    for i, data in enumerate(dataset):\n",
    "        song1, song2, label = data\n",
    "        song1, song2 = prepare_sequence(song1, word_to_ix), prepare_sequence(song2, word_to_ix)\n",
    "        label = Variable(torch.FloatTensor([label]).cuda())\n",
    "        model.hidden1 = model.initHidden(HDIM)\n",
    "        out = model(song1, song2)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = loss(out, label)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,total_loss.data[0]))\n",
    "            iteration += 10\n",
    "            if i > 0:\n",
    "                counter.append(iteration)\n",
    "                loss_history.append(total_loss.data[0])\n",
    "                avg_loss.append((sum(loss_history))/len(loss_history))\n",
    "        if i == 10000:\n",
    "            save_checkpoint({\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }, True, filename='saved_models/checkpoint'+ str(epoch) + '.pth.tar')\n",
    "            f = open(\"loss/loss\" + str(epoch) + \".txt\", \"w\")\n",
    "            [f.write(str(l)) for l in avg_loss]\n",
    "            f.close()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(counter,avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shake_it_off.txt bad_blood.txt\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9191\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataset,0):\n",
    "    song1, song2, label = data\n",
    "    song1, song2 = prepare_sequence(song1, word_to_ix), prepare_sequence(song2, word_to_ix)\n",
    "    label = Variable(torch.FloatTensor([label]))\n",
    "    model.hidden1 = model.initHidden(32)\n",
    "    out = model(song1, song2)\n",
    "    print(label)\n",
    "    print(out)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
