{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def openFile(path):\n",
    "    f = open(path, \"r\")\n",
    "    return f.read() \n",
    "\n",
    "def convertForDict(word):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    word = pattern.sub('', word)\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img,text,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SiameseLSTM(nn.Module):\n",
    "    hdim1 = 32\n",
    "#     hdim2 = 64\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, self.hdim1)\n",
    "#         self.lstm2 = nn.LSTM(hdim1, hdim2)\n",
    "        \n",
    "        self.hidden1 = self.initHidden(self.hdim1)\n",
    "#         self.hidden2 = self.initHidden(hdim2)\n",
    "        self.output = nn.Linear(self.hdim1, 1)\n",
    "\n",
    "        \n",
    "    def initHidden(self, dim):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, dim)),\n",
    "        autograd.Variable(torch.zeros(1, 1, dim)))\n",
    "        \n",
    "    def forward_once(self, lyric):\n",
    "        embeds = self.embeddings(lyric)\n",
    "        lstm_out, self.hidden1 = self.lstm1(\n",
    "            embeds.view(len(lyric), 1, -1), self.hidden1)\n",
    "        scores = self.output(lstm_out.view(len(lyric), -1))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "#         print(output1, output2)\n",
    "        out1 = output1[len(output1)-1]\n",
    "        out2 = output2[len(output2)-1]\n",
    "#         return F.softmax(-torch.abs(out1-out2))\n",
    "        return torch.abs(out1 - out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LyricDataset(Dataset):\n",
    "    def __init__(self, pathToData, numClasses, should_invert=True):\n",
    "        self.pathToData = pathToData\n",
    "        self.data = [path for path in os.listdir(self.pathToData) if os.path.isdir(self.pathToData + \"/\" + path)]\n",
    "        self.should_invert = should_invert\n",
    "        self.numClasses = numClasses\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        labels = self.data\n",
    "        sameClass = random.randint(0,1)\n",
    "        song1 = None\n",
    "        song2 = None\n",
    "        if sameClass: \n",
    "            label = random.randint(0, self.numClasses-1)\n",
    "            classPath = self.pathToData + \"/\" + labels[label]\n",
    "            songs = [path for path in os.listdir(classPath) if not path.startswith(\".\")]\n",
    "            index1 = random.randint(0,len(songs)-1)\n",
    "            s1Path = classPath + \"/\" + songs[index1]\n",
    "            index2 = index1 \n",
    "            while index2 == index1:\n",
    "                index2 = random.randint(0,len(songs)-1)\n",
    "            s2Path = classPath + \"/\" + songs[index2]\n",
    "\n",
    "        else:\n",
    "            label1 = random.randint(0, self.numClasses-1)\n",
    "            label2 = label1\n",
    "            while label2 == label1:\n",
    "                label2 = random.randint(0, self.numClasses-1)\n",
    "            classPath1 = self.pathToData + \"/\" + labels[label1]\n",
    "            songs1 = [path for path in os.listdir(classPath1) if not path.startswith(\".\")]\n",
    "            index1 = random.randint(0,len(songs1)-1)\n",
    "            s1Path = classPath1 + \"/\" + songs1[index1]\n",
    "            \n",
    "            classPath2 = self.pathToData + \"/\" + labels[label2]\n",
    "            songs2 = [path for path in os.listdir(classPath2) if not path.startswith(\".\")]\n",
    "            index2 = random.randint(0,len(songs2)-1)\n",
    "            s2Path = classPath2 + \"/\" + songs2[index2]\n",
    "            \n",
    "        s1 = openFile(s1Path)\n",
    "        lyric1 = [convertForDict(word) for word in s1.split(\" \")]\n",
    "        s2 = openFile(s2Path)\n",
    "        lyric2 = [convertForDict(word) for word in s2.split(\" \")]\n",
    "        return lyric1, lyric2, sameClass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([len(os.listdir(self.pathToData + \"/\" + p)) for p in os.listdir(self.pathToData) if p != \".DS_Store\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        return torch.abs(torch.mean(torch.abs(output1-output2))- label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LyricDataset at 0x111edad30>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/Users/alexchan/Documents/college/susa/LyricNet/train\"\n",
    "dataset = LyricDataset(PATH, 2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "word_to_ix = {}\n",
    "labels = dataset.data\n",
    "for artist in labels:\n",
    "    for song in os.listdir(dataset.pathToData + \"/\" + artist):\n",
    "        for word in openFile(dataset.pathToData + \"/\" + artist + \"/\" + song).split(\" \"):\n",
    "            word = convertForDict(word)\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = []\n",
    "iteration = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.005375061184167862\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.000732827524188906\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0002851230092346668\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.011642218567430973\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.8766337633132935\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.8902105689048767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EDIM = 32\n",
    "\n",
    "model = SiameseLSTM(EDIM, len(word_to_ix))\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, data in enumerate(dataset,0):\n",
    "        song1, song2, label = data\n",
    "        song1, song2 = prepare_sequence(song1, word_to_ix), prepare_sequence(song2, word_to_ix)\n",
    "        label = Variable(torch.FloatTensor([label]))\n",
    "        model.hidden1 = model.initHidden(32)\n",
    "        out = model(song1, song2)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = loss(out, label)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,total_loss.data[0]))\n",
    "            iteration += 10\n",
    "            counter.append(iteration)\n",
    "            loss_history.append(total_loss.data[0])\n",
    "show_plot(counter,loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
